\documentclass[twoside]{article}

\usepackage{aistats2022}
% If your paper is accepted, change the options for the package
% aistats2022 as follows:
%
%\usepackage[accepted]{aistats2022}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
%\usepackage[round]{natbib}
%\renewcommand{\bibname}{References}
%\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}
%\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage[round]{natbib}
\usepackage[inline]{enumitem}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{mwe}

\usepackage{array}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}

\usepackage{color}
\usepackage{colortbl}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{gray}{rgb}{0.7,0.7,0.7}

\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = black, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = blue  %Colour of citations
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\theoremstyle{definition}
\newtheorem{assumption}{Assumption}
\newtheorem{problem}{Problem}
\newtheorem{defn}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{fact}{Fact}

\newcommand{\R}{\mathbb R}
\DeclareMathOperator{\vcdim}{VCdim}
\DeclareMathOperator{\ddim}{c_{\text{dd}}}
\DeclareMathOperator{\E}{\mathbb E}
\DeclareMathOperator{\nnz}{nnz}
\DeclareMathOperator{\determinant}{det}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\softmax}{softmax}

\newcommand{\I}{\mathbf I}
\newcommand{\Q}{\mathbf Q}
\newcommand{\p}{\mathbf P}
\newcommand{\pb}{\bar {\p}}
\newcommand{\pbb}{\bar {\pb}}
\newcommand{\pr}{\bm \pi}
\newcommand{\epsapp}{\epsilon_{\text{app}}}
\newcommand{\epsest}{\epsilon_{\text{est}}}

\newcommand{\parent}[1]{\texttt{parent}({#1})}

\renewcommand{\star}[1]{{#1}^{*}}
\newcommand{\bad}[1]{{#1}^{\textit{bad}}}
\newcommand{\trans}[1]{{#1}^{T}}
\newcommand{\loss}{\ell}
\newcommand{\aaa}{\mathbf a}
\newcommand{\vv}{\mathbf v}
\newcommand{\uu}{\mathbf u}
\newcommand{\w}{\mathbf w}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\lone}[1]{{\lVert {#1} \rVert}_1}
\newcommand{\ltwo}[1]{{\lVert {#1} \rVert}_2}
\newcommand{\lp}[1]{{\lVert {#1} \rVert}_p}
\newcommand{\linf}[1]{{\lVert {#1} \rVert}_\infty}
\newcommand{\lF}[1]{{\lVert {#1} \rVert}_F}

\newcommand{\dist}[2]{d_{{#1},{#2}}}
\newcommand{\level}[1]{\texttt{level}({#1})}

\newcommand{\h}{\mathcal H}
\newcommand{\D}{\mathcal D}
\DeclareMathOperator*{\erm}{ERM}

%\newcommand{\fixme}[1]{\noindent{\color{red}\textbf{FIXME:}  {#1}}}
\newcommand{\fixme}[1]{}
\newcommand{\fixmemike}[1]{\noindent{\color{blue}\textbf{FIXME (Mike):}  {#1}}}

\newcommand{\ignore}[1]{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{The Tree Loss: Improving Generalization with Many Classes}

\aistatsauthor{ Yujie Wang \And Michael Izbicki  }

\aistatsaddress{ Claremont Graduate University \And  Claremont McKenna College } ]

\begin{abstract}
    %Multi-class logistic regression has generalization error $O(\sqrt{k/n})$, where $k$ is the number of classes and $n$ the number of data points.

    Multi-class classification problems often have many semantically similar classes.
    For example, 90 of ImageNet's 1000 classes are for different breeds of dog.
    We should expect that these semantically similar classes will have similar parameter vectors,
    but the standard cross entropy loss does not enforce this constraint.
    %These classes are semantically similar to each other,
    %and so we expect their learned parameter vectors to also be similar.
    %Unfortunately, the standard cross entropy loss function does not enforce this constraint.
    %It has generalization error $O(\sqrt{k/n})$, where $k$ is the number of classes and $n$ the number of data points.

    We introduce the tree loss as a drop-in replacement for the cross entropy loss.
    The tree loss re-parameterizes the parameter matrix in order to guarantee that semantically similar classes will have similar parameter vectors.
    Using simple properties of stochastic gradient descent,
    we show that the tree loss's generalization error is asymptotically better than the cross entropy loss's.
    We then validate these theoretical results on synthetic data, image data (CIFAR100, ImageNet), and text data (Twitter).

    %Whereas The standard cross entropy loss function has generalization error $O(\sqrt{k/n})$,
    %where $k$ is the number of classes and $n$ the number of data points.

%Softmax Cross Entropy loss function treats every class equally while classes often have an internal structure. 
%We introduce cover tree loss function which takes advantage of this internal class structure.
%The cover tree loss works by reparameterizing the weight matrix for the cross entropy loss.
%Compare to cross entropy which has generalization error $O(\sqrt{k/n})$, the generalization error of cover tree loss is $O(\sqrt{1/n})$ which is independent of the number of classes. 
%We provide the theoretical justification and conduct extensive experiments on fully synthetic data and real world data. 
%For real world data, we take CIFAR100 dataset, IMAGENET dataset and TWITTER dataset.
%The cover tree loss outperforms than cross entropy loss function on metrics.
\end{abstract}

\section{Introduction}

%Binary classification is perhaps the oldest and most important problem in statistical learning theory.
%The classic VC theory \citep{vapnik2015uniform} shows that for linear problems in $d$ dimensions and $n$ data points,
%the generalization error decays at a rate of $O(\sqrt{d/n})$.
%Multi-class classification is a natural generalization of the binary case,
%and \citet{natarajan1989learning} generalized the VC theory to show that for a $k$-class problem, the generalization error decays as $O(\sqrt{kd/n})$.
%This bound is tight in the sense that for any given hypothesis class,
%there exists a data distribution whose generalization error will decay as $\Omega(\sqrt{kd/n})$.
%Chapter 29 of \citet{shalev2014understanding} provides a modern overview of these results.

The cross entropy loss is the most widely used loss function for multi-class classification problems,
and stochastic gradient descent (SGD) is the most common algorithm for optimizing this loss.
Standard results show that generalization error decays at a rate of $O(\sqrt{kd/n})$,
where $k$ is the number of class labels, $d$ is the number of feature dimensions, and $n$ is the number of data points.
These results (which we review later) make no assumptions about the underlying distribution of classes.

In this paper,
we assume that the class labels have an underlying metric structure,
and we introduce the tree loss for exploiting this structure.
The tree loss is the first multi-class loss function with provably better generalization error than the cross entropy loss.
We show that if the class label metric has doubling dimension $c$,
then SGD applied to the tree loss will converge at a rate of $O(\sqrt{d \log k/n})$ when $c\le 1$ or $O(\sqrt{k^{1-1/c}d/n})$ when $c\ge 1$.
These improvements are most dramatic for small $c$,
and our empirical results show that many real world classification problems have a label structure with small $c$.

Our paper is organized as follows.
We begin in Section \ref{sec:related} by discussing limitations of related loss functions and how the tree loss will address those limitations.
Section \ref{sec:tree} then formally defines the tree loss.
We emphasize that the tree loss is simply a reparameterization of the standard cross entropy loss,
and so it is easy to implement in common machine learning libraries.
Section \ref{sec:theory} reviews standard results on the convergence of stochastic gradient descent,
and uses those results to prove the convergence bounds for the tree loss.
Finally, Section \ref{sec:experiment} conducts experiments on synthetic, real world image (CIFAR100, ImageNet) and text (Twitter) data.
We show that in practice, the tree loss essentially always outperforms the cross entropy loss.

\section{Related Work}
\label{sec:related}

Previous work on multi-class loss functions have focused on improving either the statistical or computational performance.
Statistical work includes loss functions designed for hierarchichally structured labels \citep{cesa2006incremental,wu2017hierarchical},
loss functions for improving top-$k$ accuracy \citep{lapin2016loss},
or focusing on noisy labels \citep{sukhbaatar2014training,zhang2018generalized}.
The work most similar to ours is the simloss \citep{Kobs2020SimLossCS},
which also makes a metric assumptions about the class label structure.
Our work improves on all of this statistical work by being the first to provide convergence bounds that improve on the bounds of the cross entropy loss.

Other work focuses on improving the speed of multiclass classification.
The hierarchical softmax \citep{morin2005hierarchical} is a particularly well studied variant of the cross entropy loss with many variations \citep[e.g.][]{Peng2017IncrementallyLT,Jiang2017ExplorationOT,Yang2017OptimizeHS,Mohammed2018EffectivenessOH}.
It is easily confused with our tree loss because both loss functions involve a tree structure.
The difference, however, is that the hierarchical softmax focuses on improving runtime performance,
and most variants actually sacrifice statistical performance to do so.
The tree loss, in contrast maintains the runtime performance of the standard cross entropy loss but improves the statistical performance.
Our empirical results in Section \ref{sec:experiment} show that the tree loss has much better statistical performance than the hierarchical softmax.
Other variations of the cross entropy loss include negative sampling, as used in the celebrated word2vec model \citep{mikolov2013distributed}. 
Like with the hierarchical softmax, negative sampling makes no attempt to improve statistical performance.

\ignore{
%Hierarchical classification \citep{cesa2006incremental}
%Hierarchical but without success \citep{wu2017hierarchical}
%
%Focus on top-$k$ accuracy rather than top-1 accuracy \citep{lapin2016loss}
%
%Requires custom training procedure \citep{liu2017easy}
%
%Noisy labels \citep{sukhbaatar2014training,zhang2018generalized}

When people do a multi-class classification task, the standard loss function is cross entropy.
The mechanism of cross entropy loss function is to maximize the assigned target class probability and punishes every misclassification equally, independent of some other information about the predicted class.
However, classes often have an internal structure. 
For example, dog and cat belong to same 'parent': animal and many transportations have same 'parent' while they have different brothers. Take the internal structure in the training procedure would allow the classifier to make less sever mistakes as it learns to predict similar classes.
In terms of theoretical result, the parameter matrix of cross entropy loss is bounded by $\sqrt{k}B$ (where $k$ is the number of classes) and for $n$ data points, the generalization error is $O(\sqrt{k/n})$ which is dependent on $k$.

In this paper, we introduce the tree loss which take advantage of the internal structure bewtween classes.
The idea of the tree loss is that we use tree structure to reduce the bound of the size of the parameter space and then the generalization error of the tree loss is $O(\sqrt{1/n})$ which is independent of $k$.
In the tree loss, we take a tree structure which defined by similarity between classes to re-parameterize the weight matrix for the cross entropy loss.
We incorporate class similarities with loss function to make the model learn more information about classes.
It can benefit the tasks on unbalanced datasets.
Take a emoji prediction task as an example, people often use emojis in tweets.
However, some emojis are popular and some emojis are rarely used.
This leads to an unbalanced tweets data and there is not enough training data for unpopular emojis.
The tree loss can learn more information and work well for unbalanced data.


Our contributions are as below:
\begin{itemize}
    \item [1] 
    We introduce the tree loss which take advantage of internal structure between all classes.
    \item [2] 
    We provide theoretical justification that the generalization error of the tree loss is independent of the number of classes.
    \item [3]
    We evaluate our tree loss on experiments. One is experiment on fully synthetic data which is designed to be convex. The another one is on real world data: CIFAR100, IMAGENET and TWITTER dataset. 
    
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
\resizebox{\columnwidth}{!}{
\begin{tikzpicture}
    [ level distance=1.5cm
    , level 1/.style={sibling distance=3cm}
    , level 2/.style={sibling distance=1.5cm}
    ]
\node[draw, rounded corners=0.1in, inner sep=0.1in] at (-1.5in,0){\textbf{$U$-Tree}};
\node {\texttt{truck}}
    child {node {\texttt{boxer}}
      child {node {\texttt{tiger}}}
      child {node {\texttt{skunk}}}
      child {node {\texttt{boxer}}
        child {edge from parent[draw=none]} % Added
        child {node {\texttt{bulldog}}}
        child {node {\texttt{boxer}}}
        child {node {\texttt{husky}}}
        child {node {\texttt{sheepdog}}}
      }
      child {node {\texttt{bear}}}
      child {edge from parent[draw=none]} % Added
      }
    child {node {\texttt{truck}}
      child {edge from parent[draw=none]} % Added
      child {node {\texttt{truck}}}
      child {node {\texttt{bus}}}
    }
    child {node {\texttt{toaster}}}
    ;

\end{tikzpicture}
}
~~~
\resizebox{\columnwidth}{!}{
\begin{tikzpicture}
    [ level distance=1.5cm
    , level 1/.style={sibling distance=3cm}
    , level 2/.style={sibling distance=1.5cm}
    ]
\node[draw, rounded corners=0.1in, inner sep=0.1in] at (-1.5in,0){\textbf{$V$-Tree}};
\node {\textit{pseudo1}}
    child {node {\textit{pseudo2}}
      child {node {\texttt{tiger}}}
      child {node {\texttt{skunk}}}
      child {node {\textit{pseudo3}}
        child {edge from parent[draw=none]} % Added
        child {node {\texttt{bulldog}}}
        child {node {\texttt{boxer}}}
        child {node {\texttt{husky}}}
        child {node {\texttt{sheepdog}}}
      }
      child {node {\texttt{bear}}}
      child {edge from parent[draw=none]} % Added
      }
    child {node {\textit{pseudo4}}
      child {edge from parent[draw=none]} % Added
      child {node {\texttt{truck}}}
      child {node {\texttt{bus}}}
    }
    child {node {\texttt{toaster}}}
    ;

\end{tikzpicture}
}
    \caption{
        Example label tree structures for a subset of 10 ImageNet classes.
        The $U$-tree has class labels that repeat at multiple levels,
        and the $V$-tree introduces ``pseudoclasses''.
        The pseudoclass \textit{pseudo3} represents the class of ``dogs'',
        and the pseudoclass \textit{pseudo2} represents the class of ``animals''.
    }
    \label{fig:labeltree}
\end{figure*}

\section{The Tree Loss}
\label{sec:tree}

This section first formally defines our problem setting,
then formally defines the tree loss.
We conclude with a comparison between the tree loss and related loss functions.

\subsection{Problem Setting}

We consider the multi-class classification setting with $k$ classes and $d$ input feature dimensions.
The cross entropy loss is the standard loss function for this setting.
It is defined to be
\begin{equation}
    \label{eq:xentropy}
    \ell(W;(\x,y)) = - \log \frac {\exp(-\trans\w_y \x)}{\sum_{j=1}^k \exp(-\trans \w_j \x)}
\end{equation}
where for each class $i\in[k]$,
$\w_i : \R^d$ is the parameter vector associated with class $i$;
the variable $W : \R^{k \times d} = (\w_1; \w_2; ...; \w_k)$ is the full parameter matrix;
$\x : \R^d$ is the input feature vector;
and $y \in [k]$ is the input class label.

\subsection{The Tree Loss}

The tree loss reparameterizes the weight matrix used in the cross entropy loss in order to enforce that semantically similar labels will have similar weight vectors.
The reparameterization is defined according to a \emph{label tree} structure that captures the similarity between labels.
And we propose two variants of the tree loss called the $U$ and $V$ variants that use slightly different label tree structures.
The main purpose of the $U$-tree structure is to simplify the analysis and exposition of the $V$-tree structure,
and we show later in our theoretical results that the $V$-tree is strictly better than the $U$-tree.
Figure \ref{fig:labeltree} shows examples of both.

\subsubsection{The $U$-Tree Loss}

In a $U$-tree, every node is a class and the class of all internal nodes matches the class of one of its children.
For each class $i$, we define $\parent{i}$ to be the parent of the upper-most node for class $i$,
and the $\texttt{parent}$ of the root is the 0 vector..
For example, in the example of Figure \ref{fig:labeltree},
we have
\begin{equation*}
\begin{split}
    \parent{\texttt{husky}} &= \texttt{boxer} \text{, and}\\
    \parent{\texttt{bear}} &= \texttt{boxer} \text{, and}\\
    \parent{\texttt{boxer}} &= \texttt{truck} .\\
\end{split}
\end{equation*}
We associate for each class $i$ a new parameter vector $\uu_i$ and set
\begin{equation}
    \w_i = \w_{\parent{i}} - \uu_i,
    \label{eq:uu}
\end{equation}
and we define the parameter matrix $U : \mathbb R^{k\times d}$ to be $(\uu_1;...;\uu_k)$.
The $U$-tree loss is then defined by substituting \eqref{eq:uu} into \eqref{eq:xentropy} to get
\begin{equation*}
    \ell(U;(\x,y)) = - \log \frac {\exp(-(\trans\uu_{\parent{y}}-\trans\uu_y) \x)}{\sum_{j=1}^k \exp(-(\trans\uu_{\parent{j}}-\trans \uu_j) \x)}
\end{equation*}
We use the same $\ell$ notation for the standard cross entropy loss function and the $U$-Tree loss because the function is exactly the same;
the only difference is how we represent the parameters.

\subsubsection{The $V$-Tree Loss}

The $V$-tree is constructed from the $U$-tree by replacing all non-leaf nodes with ``pseudoclasses''.
Let $k'$ be the total number of nodes in the $V$-tree;
so $k'$ equals $k$ plus the number of internal nodes.
For each class $i$, we let the sequence $P_i$ denote the path from the leaf node to the root.
For example, using the $V$-tree in Figure \ref{fig:labeltree},
we have
\begin{equation*}
\begin{split}
    P_{\texttt{sheepdog}} &= (\texttt{sheepdog}, \textit{pseudo3}, \textit{pseudo2}, \textit{pseudo1} ) \\
    P_{\texttt{truck}} &= (\texttt{truck}, \textit{pseudo4}, \textit{pseudo1} )
\end{split}
\end{equation*}
Next, we associate a vector $\vv_i$ to each node $i$ in the tree structure.
In particular, there is a vector $\vv_i$ for each class and for each pseudo-class,
so there are more parameter vectors for the $V$-tree loss than either the $U$-tree loss or the standard cross entropy loss.

Finally, we reparameterize the weights for the cross entropy loss as the sum of the vectors along the paths.
Specifically, we set the weight vector for class $i$ to be 
\begin{equation}
    \label{eq:covertree-reparam}
    \w_j = \sum_{j\in P_i} \vv_i
    .
\end{equation}
The $V$-tree loss is defined by substituting \eqref{eq:covertree-reparam} into \eqref{eq:xentropy} to get
\begin{equation}
    \ell(V;(\x,y)) = - \log \frac {\exp(-\sum_{k\in P_y}\trans\vv_k \x)}{\sum_{j=1}^k \exp(- \sum_{k\in P_j}\trans\vv_k \x)}
    .
\end{equation}
We continue to use the $\ell$ function to represent both the standard cross entropy loss and the $V$-tree loss function to emphasize that these are the same loss functions,
just with different parameterizations of the weight matrix.

\subsection{Intuition}

The intuition behind the $V$-tree reparameterization is that whenever we perform a gradient update on a data point with class $i$,
we will also be ``automatically'' updating the parameter vectors of similar classes.
To see this, note that when we have a \texttt{sheepdog} data point,
we will perform gradient updates on all $\vv_i$ in $P_\texttt{sheepdog}$; i.e. $\vv_{sheepdog}$, $\vv_{\textit{pseudo3}}$, $\vv_{\textit{pseudo2}}$, and $\vv_{\textit{pseudo1}}$.
This will cause the $\w_{\texttt{husky}}$ and $\w_{\texttt{bear}}$ parameters (among many others) to update because they also depend on the pseudoclass vectors $\vv_{\textit{pseudo3}}$ and $\vv{_\textit{pseudo2}}$.
Because $P_\texttt{husky}$ has a larger overlap with $P_\texttt{sheepdog}$ than $P_\texttt{bear}$,
the parameters of this more similar class will be updated more heavily.

\subsection{Implementation Notes}

Both the $U$-tree and $V$-tree losses are easy to implement in practice using the built-in cross entropy loss function in libraries like PyTorch \citep{NEURIPS2019_9015} or Tensorflow \citep{tensorflow2015-whitepaper}.
The only change needed is to define the loss in terms of the sum of vectors $V$ instead of a variable tensor $W$.
This ensures that the loss is implemented efficiently and is numerically stable.

In practice, the $V$-tree loss is slightly slower than the standard cross entropy loss and $U$-tree losses due to the additional summation over the paths.
In our experiments, we found this this to cause the $V$-tree loss to be about twice as slow.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Theoretical Results}
\label{sec:theory}

It is well known that when using the standard cross entropy loss,
SGD converges at a rate of $O(\sqrt{k/n})$.
Our theoretical results asymptotically improve this convergence rate.

With an appropriately constructed label tree,
we show convergence at a rate of $O(\sqrt{\log k/n})$ or $O(\sqrt{k^{1-1/c}/n})$,
where $c$ is a measure of difficulty of the problem.
But what happens if the tree is poorly constructed?
Remarkably, nothing bad.
We also show that for arbitrarily bad label tree structures,
the $U$ and $V$ losses maintain a $O(\sqrt{k/n})$ convergence rate.
This suggests that there is essentially no downside to using the tree loss parameterizations.

\fixme{
Our analysis relies on simple properties of stochastic gradient descent,
and we view the simplicity of our analysis as a major strength of the tree loss.
In this section, we first review standard results of stochastic gradient descent needed for our analysis.
Then we formally state and prove our main result.
}

\subsection{SGD Background}

SGD is a standard algorithm for optimizing convex losses,
and it has many variations.
Our analysis uses the results from Chapter 14 of \citet{shalev2014understanding} which are based off of a version of SGD that uses parameter averaging.\footnote{
Other variants of SGD have similar results,
and our results are easy to generalize to other SGD variants.
We refer the reader to \citet{shalev2014understanding} for a discussion of these variants.}
In this section we recall these results,
occasionally changing the names of variables to avoid conflicts with variable names used elsewhere in this paper.

We assume we are optimizing a function $f : \Theta \to \mathbb R$,
and that we are only given noisy samples of the function and its gradient.
We let $\theta^{t}$ denote the parameter value at iteration $t$,
and $\eta : \mathbb R$ be the step size.
Then the parameter update for step $t$ is defined to be
\begin{equation}
    \theta^{(t+1)} = \theta^{(t)} - \eta \nabla f(\theta^{(t)}).
\end{equation}
When using parameter averaging, the final model parameters are defined to be
\begin{equation}
    \bar\theta = \frac1T \sum_{i=1}^T \theta^{(t)}
    .
\end{equation}
Parameter averaging is not often used in practice because of the memory requirements,
but we use it here due because theorems using parameter averaging are typically much easier to state and prove.
Equivalent results without parameter averaging typically require and additional $\log T$ factor in their convergence rates\citep{}.

The main result we rely on is a bound on the convergence rate of SGD for convex Lipschitz functions.
We reproduce Theorem 14.8 of \citet{shalev2014understanding} in the theorem below.

\begin{theorem}
    \label{thm:sgd}
    Let $\beta,\rho>0$.
    Let $f$ be a convex function and let $\star \theta = \argmin_{\theta : \ltwo{\theta}\le \beta} f(\theta)$.
    Assume that SGD is run for $T$ iterations with $\eta = \sqrt{\frac{\beta^2}{\rho^2 T}}$.
    Assume that for all $t$, $\ltwo{\nabla f(\theta^{(t)})} \le \rho$.
    Then,
    \begin{equation}
        \E f(\bar \theta) - f(\star \theta) \le \frac{\beta\rho}{\sqrt{T}}
        .
    \end{equation}
\end{theorem}

\subsection{SGD and Statistical Learning}

We now introduce more statistical learning theory notation,
then apply Theorem \ref{thm:sgd} above to bound the rate of learning.

Define the true loss of our learning problem as
\begin{equation}
    L_D(A) = \E_{(\x,y)\sim D} \ell(A; (\x, y))
\end{equation}
where $A \in \{W, U, V\}$ is a parameterization of the cross entropy loss.
The optimal parameter vector is defined to be the minimum of the true loss:
\begin{equation}
    \star A = \argmin_{A} L_D(A).
\end{equation}
In order to apply Theorem \ref{thm:sgd} to a learning problem,
we make the following assumption about our the size of our data points:
\begin{assumption}
    \label{ass:lip}
    For each feature vector $\x$ in the data set, 
        $\ltwo{\x} \le \rho$
        .
\end{assumption}
Under this assumption, the cross entropy loss $\ell$ is $\rho$-Lipschitz for all parameterizations,
and the true loss $L_D$ is therefore also $\rho$-Lipschitz.

Applying Theorem \ref{thm:sgd} with $f=L_D$ gives
\begin{corollary}
    \label{ref:cor:A}
    Under Assumption \ref{ass:lip}, we have that for any parameterization $A$ of the cross entropy loss,
    \begin{equation}
        \E L_D(\bar A) - L_D(\star A) \le \frac{\lF{\star A}\rho}{\sqrt n}.
        \label{eq:Aconv}
    \end{equation}
\end{corollary}

We can now bound the convergence of the standard cross entropy loss and the $U$ and $V$-tree losses by bounding the Frobenius norms of their parameter matrices.

In order to recover the standard convergence rate of multiclass classification, we make the following assumption.
\begin{assumption}
    \label{ass:B}
    For each class $i$, the optimal parameter vector $\star\w_i$ satisfies
        $\ltwo{\star\w_i} \le B$
        .
\end{assumption}
Note that this assumption is not limiting because the value of $B$ need not be known during the optimization procedure.

It follows that
\begin{equation}
    \lF{\star W}^2 = \sum_{i=1}^k \ltwo{\star\w_i} \le kB^2.
    \label{eq:starW}
\end{equation}
Substituting Eq \eqref{eq:starW} into \eqref{eq:Aconv} gives the following bound.

\begin{theorem}
\label{theorem:xentropy}
    Under assumptions \ref{ass:lip} and \ref{ass:B} above,
then the parameter vector $\bar W$ estimated from one epoch of SGD satisfies
\begin{equation}
    \E L_D(\bar W) - L_D(W^*)
    \le \frac {\sqrt kB\rho}{\sqrt n}
    .
\end{equation}
\end{theorem}
\fixme{
It is important to note that this result above is an upper bound and not a lower bound.
This $O(\sqrt{k/n})$ convergence rate matches the convergence rate implied by the Nataranj dimension \citep{}.
We are not aware of any lower bounds for SGD in the Lipschitz setting.
\citet{nguyen2018tight,jentzen2020lower} study the strongly convex case and find that the upper bounds match the lower bounds.
}

\subsection {Bad Tree Structures}

\begin{lemma}
    \label{lemma:starV}
    We have that for all $V$-trees derived from a $U$-tree,
    \begin{equation}
        \lF{\star V} \le \lF{\star U}.
    \end{equation}
\end{lemma}
\fixme{
\begin{proof}
\end{proof}
}

The following lemma shows that even for arbitrarily bad tree structures,
the bound on the parameter spaces cannot be too bad.

\begin{lemma}
    \label{lemma:starU}
    Under assumption \ref{ass:B},
    the following bound holds for all $U$-tree structures:
    \begin{equation}
        \lF{\star U} \le 2\sqrt{k}B.
    \end{equation}
\end{lemma}
\begin{proof}
    We have
    \begin{align}
        \lF{\star U}^2 
        &= \sum_{i=1}^k \ltwo{\star\uu_i}^2 \\
        &= \sum_{i=1}^k \ltwo{\star\w_i - \star\w_{\parent{i}}}^2 \\
        &\le \sum_{i=1}^k \left(\ltwo{\star\w_i} + \ltwo{\star\w_{\parent{i}}} \right)^2 \\
        &\le \sum_{i=1}^k (2B)^2 \\
        &\le k (2B)^2
        .
    \end{align}
\end{proof}

\subsection{Good Tree Structures}

\begin{assumption}
We are given a distance metric $d$ over the labels such that for all labels $i$ and $j$,
\begin{equation}
    \frac 1 \lambda d(i,j)
    \le \ltwo{\star \w_i - \star \w_j}
    \le \lambda d(i, j).
\end{equation}
We denote by $c$ the doubling dimension of the metric $d$.
\end{assumption}

\begin{lemma}
    \label{lemma:main}
    When $c\le1$, we have that
    \begin{equation}
        \lF{\star U} \le \tfrac{1}{\sqrt2}\lambda B \sqrt{\log_2 k}.
        \label{eq:c<=1}
    \end{equation}
    When $c>1$, we have that
    \begin{equation}
        \lF{\star U} \le \sqrt{5}\lambda B \sqrt{k^{(1-1/c)}}.
        \label{eq:c>1}
    \end{equation}
\end{lemma}

\fixme{
\begin{proof}
    First, we review some simple properties of the cover tree structure that will be needed in our proof.
    Let $q$ denote the maximum number of children of any node in the cover tree.
    A standard property of cover trees is that $q \le (4^c)$ \citep[Lemma 25]{izbickithesis}.
    Let $Q_i$ denote the number of nodes at level $i$ in the cover tree.
    Then we have that $|Q_i| \le q^i.$
    Finally, let $r$ be the height of the cover tree.
    Without loss of generality, we analyze the bound when the cover tree is perfectly balanced and so $r = \log_q k = \log_2 k / (2c)$.

We have that
\begin{align}
    \lF{\star U}^2
    &= \sum_{i=1}^k \ltwo{\star\uu_i}^2 \\
    &= \sum_{i=0}^r \sum_{j\in Q_i} \ltwo{\star\uu_j}^2 \\
    &= \sum_{i=0}^r \sum_{j\in Q_i} \ltwo{\star\w_j - \star\w_{\parent{j}}}^2 \\
    &\le \sum_{i=0}^r \sum_{j\in Q_i} \lambda^2 d(j, \parent{j})^2 \\
    &\le \sum_{i=0}^r \sum_{j\in Q_i} \lambda^2 (B/2^i)^2 \\
    &\le \sum_{i=0}^r q^i \lambda^2 (B/2^i)^2 \\
    &\le \sum_{i=0}^r \lambda^2 B^2 (q/4)^i 
    \label{eq:precases}
\end{align}
We proceed by considering the case of $c\le1$ and $c>1$ separately.

When $c\le1$, $q\le4$.
Substituting into Eq \eqref{eq:precases}, we have
\begin{align}
    \lF{\star U}^2
    &\le \sum_{i=0}^r \lambda^2 B^2 \\
    &= (r+1) \lambda^2 B^2 \\
    &= \frac{\log_2k}{2c} \lambda^2 B^2 \\
    &\le \frac{\log_2k}{2} \lambda^2 B^2
    .
\end{align}
This matches Eq \eqref{eq:c<=1} in the lemma's statement.

When $c>$, then $q>4$.
We first show the following equality.
\begin{align}
\left(\frac q 4\right)^r
&=
\left(\frac q 4\right)^{\log_q k}\\
&=
\frac{k}{4^{\log_q k}}\\
&=
\frac{k}{4^{{\log_4 k}/{\log_4 q}}}\\
&=
\frac{k}{k^{1/\log_4 q}}\\
&=
k^{\left(1 - {1}/{\log_4 q}\right)} \\
&=
k^{\left(1 - 1/c\right)}
.
\label{eq:lFU:frac}
\end{align}
Substituting into Eq \eqref{eq:precases}, we have
\begin{align}
    \lF{\star U}^2
    &= \lambda^2B^2 \left(\frac{(q/4)^{r+1}-1}{q/4-1}\right) \\
    \label{eq:lFU:4}
    &= \lambda^2B^2 \left(\frac{(q/4)k^{(1-1/c)}-1}{q/4-1}\right) \\
    &\le \lambda^2B^2 \left(\frac{(q/4)k^{(1-1/c)}}{q/4-1}\right) \\
    &\le 5 \lambda^2B^2 k^{(1-1/c)}
    \label{eq:lFU:5}
\end{align}
    Inequality \eqref{eq:lFU:5} follows because $\tfrac{q/4}{q/4-1} \le 5$ for integer values of $q>4$.
\end{proof}
}

%Substituting \eqref{eq:lFU:frac} info \eqref{eq:lFU:5} gives
%\begin{align}
    %\lF{U} \le \lambda B \sqrt{\frac{\log_2 k}{2c} + 1}\sqrt{k^{(1-1/c)}}
%\end{align}
%\begin{lemma}
    %\label{lemma:technical}
    %For all $a>1$ and $b>1$, we have that
    %\begin{equation}
        %\frac {1-a^b}{1-a} \le ba^b
    %\end{equation}
%\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}
\includegraphics[width=\columnwidth,height=1.5in]{fig/ima/accuracy_vs_n.png}
\includegraphics[width=\columnwidth,height=1.5in]{fig/ima/accuracy_vs_d.png}
\includegraphics[width=\columnwidth,height=1.5in]{fig/ima/accuracy_vs_class.png}
\includegraphics[width=\columnwidth,height=1.5in]{fig/ima/accuracy_vs_sigma.png}
\caption{
    Results of our Synthetic Experiments I.
    The x-axis of each plot shows which problem parameter is being varied.
    As our theory predicts, the Tree Loss outperforms the baseline loss functions in all data regimes.
}
\label{fig:synth:1}
\end{figure*}

\section{Experiments}
\label{sec:experiment}

\subsection{Synthetic data}

In this experiment we will demonstrate the tree loss's effectiveness on a simple multiclass logistic regression problem.
The problem will be specifically designed to be convex and have low intrinsic dimension in the class labels in order to illustrate the advantages of the cover tree loss.

\subsubsection{Data Generation Procedure}
\label{sec:exp:synth:problem}

Let $\mathcal N$ be the standard normal distribution.
Then sample the true parameter matrix
\begin{align}
    \star W \sim \mathcal N^{k\times d}
    .
\end{align}
For all $i \in [k]$, the row $\w_i^*$ of $W^*$ corresponds to the parameter vector for class $i$ and has dimension $d$.

For each data point $i\in[n]$,
we sample the data point according to the following rules:
\begin{align}
    y_i &\sim \text{Uniform}([k]), \text{and} \\
    \x_{i} &\sim \mathcal N(\w^*_{y_i}; \sigma),
\end{align}
where $\sigma$ controls the noise in the dataset.
Increasing $\sigma$ adds noise and makes classes harder to distinguish,
whereas decreasing $\sigma$ reduces noise and makes the classes easier to distinguish.

With our problem defined in this way, it is clear that the empirical risk minimizer
\begin{equation}
    \hat W = \argmin_{W} \tfrac 1 n \ell(W; \x_i, y_i)
\end{equation}
<<<<<<< HEAD
converges to $W^*$ using the standard cross entropy loss and both the matrix factorization and tree reparameterizations.
% \fixme{You should prove this for yourself.}
% Figure \ref{fig:synthetic-results} shows that the cover tree loss converges at a faster rate.

\begin{figure}[h]
\vspace{.3in}
\includegraphics[width=\linewidth]{fig/images/accuracy_vs_n.png}
\vspace{.3in}
\caption{\small Accuracy versus number of data points($d=64, \sigma=1.0, k=10$)}
\label{accuracy_vs_n}
\end{figure}

\begin{figure}[h]
\vspace{.3in}
\includegraphics[width=\linewidth]{fig/images/accuracy_vs_d.png}
\vspace{.3in}
\caption{\small Accuracy versus dimension of parameter matrix($n=100, \sigma=1.0, k=10$)}
\label{accuracy_vs_d}
\end{figure}

\begin{figure}[h]
\vspace{.3in}
\includegraphics[width=\linewidth]{fig/images/accuracy_vs_sigma.png}
\vspace{.3in}
\caption{\small Accuracy versus randomness($n=100, d=64, k=10$)}
\label{accuracy_vs_sigma}
\end{figure}

\begin{figure}[h]
\vspace{.3in}
\includegraphics[width=\linewidth]{fig/images/accuracy_vs_class.png}
\vspace{.3in}
\caption{\small Accuracy versus number of classes($n=100, d=64, \sigma=1.0$)}
\label{accuracy_vs_class}
\end{figure} 

\begin{figure}[h]
\vspace{.3in}
\includegraphics[width=\linewidth]{fig/new_img/accuracy_vs_base.png}
\vspace{.3in}
\caption{\small Accuracy versus base of tree($n=1000, d=10, \sigma=1.0, k=100$)}
\label{accuracy_vs_base}
\end{figure}

\begin{figure}[h]
\vspace{.3in}
\includegraphics[width=\linewidth]{fig/new_img/height_vs_base.png}
\vspace{.3in}
\caption{\small Height versus base of tree($n=1000, d=10, \sigma=1.0, k=100$)}
\label{height_vs_base}
\end{figure}

\begin{figure}[h]
\vspace{.3in}
\includegraphics[width=\linewidth]{fig/new_img/loss_vs_structure.png}
\vspace{.3in}
\caption{\small Accuracy versus tree structure($n=1000, d=64, \sigma=1.0, k=100$)}
\label{accuracy_vs_structure}
\end{figure}


\subsubsection{Results}
It is clear that the tree loss achieves the best performance. 
From figure \ref{accuracy_vs_n} and \ref{accuracy_vs_d}, the tree loss converges at a faster rate as number of data points and dimension of parameter matrix increase.
Figure \ref{accuracy_vs_class} shows that if we increase the number of classes, the tree loss can keep a higher accuracy and the accuracy of SimLoss is higher than cross entropy loss and hierarchical softmax.
=======
converges to $W^*$.


\subsubsection{Experiment I: Data Regimes}

In this experiment, we investigate the tree loss's performance in a wide range of settings as we vary the number of data points ($n$), the number of dimensions ($d$), number of class labels ($k$), and the Bayes error ($\sigma$) of our problem.
First, we construct a standard experimental setting with $n=100$, $d=64$, $k=10$, and $\sigma=1$.
Then, for each of these parameters, we investigate the effect of that parameter's performance on the problem by varying it over a large range.
For each value in the range, we:
(1) randomly sample 50 problems using the procedure described in Section \ref{sec:exp:synth:problem};
(2) for each problem, we:
(2a) sample a training set with $n$ data points and a test set with 10000 data points;
(2b) train 4 models on the training set using the standard cross entropy loss, tree loss, simloss, and hierarchical softmax;
(2c) report the average accuracy accross the sampled test sets.

Figure \ref{fig:synth:1} shows the results of these experiments.
As our theory suggests, the tree loss outperforms all other losses in all regimes.
Consider the top-left plot.
As the number of data points $n$ gets large,
then all of the models have accuracies that converge to 1;
for most problems, however, the number of data points will not be sufficiently large to drive the generalization error to 0,
and the fewer data points that the problem has,
the better the tree loss performs compared to other losses.

\fixme{
From figure \ref{fig.1}, it is clear that the tree loss achieves the best performance. 
The tree loss converges at a faster rate as number of data points and dimension of parameter matrix increase.
If we increase the number of classes, the tree loss can keep a higher accuracy and the accuracy of SimLoss is higher than cross entropy loss and hierarchical softmax.
>>>>>>> 6ebdbf92f95244db43f83ed072ca2b7e89ea1aae
It is as our expected, the tree loss can improve performance on large classes tasks.
From figure \ref{accuracy_vs_sigma}, we can see that when we increase the noise in the dataset, the accuracy of all loss functions decrease, yet the tree loss decreases slower.
Among the four loss functions, hierarchical softmax hurts the performance obviously. 
We would not apply hierarchical softmax on real world data experiment in terms of computation.
}

<<<<<<< HEAD
There is a parameter 'base' which can control the height of tree.
Figure \ref{height_vs_base} shows that if we increase the value of base, the tree is shorter.
As our expected, from figure \ref{accuracy_vs_base} we can see that a taller tree has higher accuracy. 
We add a parameter randomness into the tree structure and the tree structure become more random as the randomness factor increase.
As figure \ref{accuracy_vs_structure} shows, when the tree structure become totally random, the performance of the tree loss is close to cross entropy loss.
=======
\subsubsection{Experiment II: Parameter Norms}
>>>>>>> 6ebdbf92f95244db43f83ed072ca2b7e89ea1aae

Corollary \ref{ref:cor:A} suggests that the norm of the parameter matrix is what controls convergence rate of SGD,
and the proof of our main result in Theorem \ref{} relies on bounding this norm.
In this experiment, we directly measure the norm of the parameter matrices and show that $\lF{V}$ is significantly less than the $lF{W}$,
justifying the good convergence rates we observed in Synthetic Experiment I above.
We set $n=1000$, $d=10$, and $\sigma=1$, and vary the number of classes $k$.
Figure \ref{fig:synth:norm} shows that $\lF{\star V}$ grows much slower than $\lF{\star W}$.
Notice in particular that $\lF{\star W}$ grows at the rate $O(\sqrt{k})$ and $\lF{\star V}$ grows at a rate strictly less than $o(\sqrt{k})$ as our theory predicts.

\begin{figure}
\includegraphics[width=\columnwidth,height=1.5in]{fig/images/class_v_norm.png}
\caption{As we increase the number of classes $k$,
    $\lF{\star V}$ grows significantly slower than $\lF{\star W}$.
    Since the convergence rate of SGD is proportional to $\lF{\cdot}$ (by Corollary \ref{ref:cor:A}),
    the tree loss converges faster than the standard cross entropy loss. 
    }
\label{fig:synth:norm}
\end{figure}


\subsubsection{Experiment III: Tree Shapes}

In this experiment, we study how the shape of our tree structure impacts performance.
We set $n=1000$, $d=10$, $k=100$, and $\sigma=1$.
We use a relatively large number of classes $k$ compared to our standard problem in Section \ref{sec:synth:1} above in order to ensure that we have enough classes to generate meaningfully different tree structures.
The basic trends we observe are consistent for all other values of $n$, $d$, and $\sigma$.

Recall that the cover tree has a parameter \texttt{base} which controls the rate of expansion between layers of the tree.
As seen in Figure \ref{fig:ct:height}, reducing \texttt{base} increases the height of the tree and increasing \texttt{base} reduces the height of the tree.
This will affect the performance of our tree loss because taller trees result in more parameter sharing.
Figure \ref{fig:ct:acc} plots the accuracy of the tree loss as a function of the \texttt{base} hyperparameter.
Across all ranges, the tree loss outperforms the cross entropy loss.
Interestingly, the tree loss's accuracy is maximized when $\texttt{base}\approx1.3$.
The original cover tree paper \citep{beygelzimer2006cover} also found that a \texttt{base} of 1.3 resulted in the fastest nearest neighbor queries.
It's unclear why 1.3 has this good quality,
but the fact that this constant results in good computational performance for nearest neighbor queries and good statistical performance in the tree loss, two rather different use cases, suggests that there is something fundamental going on here that we don't yet fully understand.

<<<<<<< HEAD
\subsubsection{CIFAR-100 Data}
We use CIFAR-100 dataset \cite{Krizhevsky2009LearningML} which has 100 classes for image classification.
We also get word embeddings from a word2vec model pretrained on Google News \cite{Mikolov2013EfficientEO} of class names and construct tree structure and calculate similarity between classes from word embeddings.
Four class names do not yield a word embedding and are therefore eliminated.
=======
\begin{figure}
            \includegraphics[width=\columnwidth,height=1.5in]{fig/new_img/height_vs_base.png}
            \caption{The height of a cover tree varies inversely with it's \texttt{base} hyperparameter.}
            \label{fig:ct:height}
\end{figure}
>>>>>>> 6ebdbf92f95244db43f83ed072ca2b7e89ea1aae

\begin{figure}
    \includegraphics[width=\columnwidth,height=1.5in]{fig/new_img/accuracy_vs_base.png}
            \caption
            {The tree loss outperforms the standard cross entropy loss for all values of \texttt{base}, but the performance is maximized when $\texttt{base}\approx1.3$.}
            \label{fig:ct:acc}
\end{figure}



\subsubsection{Experiment IV: Metric Quality}
\label{sec:synth:eps}

Recall that our theoretical results state that the convergence rate of SGD depends on a factor $\lambda$ that measures how well our metric over the class labels is able to predict the distances between the true parameter vectors.
In this experiment, we study the effect of this $\lambda$ parameter on prediction accuracy.

We fix the following problem hyperparameters:
$n=100$, $d=64$, $k=10$, and $\sigma=1$.
Then we construct a bad parameter matrix $\bad W$ using the same procedure we used to construct the optimal parameter matrix;
that is,
\begin{equation}
    \bad W \sim \mathcal N ^ {k\times d}
    .
\end{equation}
Each row $\bad \w_i$ of $\bad W$ is now a $d$ dimensional random vector that has absolutely no relation to the true parameter vector.
Next, we define a family of ``$\epsilon$-bad'' parameter vectors that mix between the perfectly bad and perfectly good parameter vectors:
\begin{equation}
    \w^\epsilon_i = (1-\epsilon) \w^*_i + \epsilon \bad\w_i.
\end{equation}
Finally, we define our $\epsilon$-bad distance metric as
\begin{equation}
    d_\epsilon(i,j) = \ltwo{\w_i^\epsilon - \w_j^\epsilon}
\end{equation}
and build our cover tree structure using $d_\epsilon$.
When $\epsilon=0$, this cover tree structure will be the ideal structure and $\lambda$ will be equal to 1;
when $\epsilon=1$, this cover tree structure will be maximally bad, and $\lambda$ will be large.

Figure \ref{fig:synth:eps} shows the performance of the tree loss as a function of $\epsilon$.
Remarkably, the tree loss outperforms the standard cross entropy loss even when using a perfectly bad tree structure (i.e. when $\epsilon=1$).
This surprising empirical finding actually agrees with our theoretical results in two ways.
First, Lemmas \ref{lemma:starV} and \ref{lemma:starU} together imply that that $\lF{\star V} = O(\lF{\star W})$,
and so no tree structure can perform asymptotically worse than the standard cross entropy loss.
Second, when $\epsilon=1$ and we have a perfectly random tree structure,
the distance $d_\epsilon$ can still be embedded into the ideal metric $d$ with some large (but finite!) $\lambda$.
Asymptotically, Lemma \ref{lemma:main} then implies that SGD converges at the rate $O(\sqrt{k^{1-1/c}})$,
which is less than the convergence rate of the standard cross entropy loss.
\fixme{replace lemma with theorem}

\begin{figure}
\includegraphics[width=\columnwidth,height=1.5in]{fig/new_img/loss_vs_structure.png}
\caption{
As $\epsilon\to1$, the quality of our metric for generating the tree structure decreases.
But even with a perfectly random tree structure,
the tree loss still outperforms the cross entropy loss.
}  
\label{fig:synth:eps}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Real World Data}

\begin{table*}
    \centering
    \input{fig/real_world.tex}
    \caption{Experimental results on real world datasets.  For all performance measures, larger numbers are better.  The tree loss achieves the best results in all cases.}
    \label{table:results}
\end{table*}

We now validate the tree loss on real world image (CIFAR100, ImageNet) and text (Twitter) data.
For each dataset, we report standard top-k accuracy scores,
and the less well known \emph{superclass accuracy} (SA) score.
SA is a generalization of the top-1 accuracy proposed by the simloss paper \citep{Kobs2020SimLossCS}.
The idea is that misclassifying a \texttt{sheepdog} as a \texttt{husky} should be penalized less than misclassifying a \texttt{sheepdog} as a \texttt{bus} because the \texttt{sheepdog} and \texttt{husky} classes are more similar to each other.

Table \ref{table:results} shows that on each of these datasets and for each metric,
the tree loss outperforms the baseline cross entropy loss and simloss.\footnote{We do not evaluate against the hierarchical softmax on these datasets for computational reasons.  The results on the synthetic data, however, suggest that the hierarchical softmax would have performed poorly.}
The remainder of this section describes the experimental procedures for each dataset in detail.

\subsubsection{CIFAR100}

CIFAR100 is a standard image dataset with 100 classes \citep{krizhevsky2009learning}.
It is more difficult than MNIST and CIFAR-10,
but small and so less computationally demanding than ImageNet.

We follow the procedure used in the SimLoss paper \citep{Kobs2020SimLossCS} in this experiment.
We generate our distance metric over the class labels using a word2vec model \citep{Mikolov2013EfficientEO} pretrained on GoogleNews.
The distance between class labels is defined to be the distance between their corresponding word2vec embeddings.
There are 4 class labels with no corresponding word2vec embedding,
and so following the simloss paper we discard these classes.

We train 3 ResNet20 models on CIFAR100 \citep{He2016DeepRL} using hyperparameters specified in the original paper.
The only difference between the three models is the final layer:
one model uses the standard cross entropy loss, one uses the tree loss, and one uses the simloss.
The results shown in Table \ref{table:results} shows that the tree loss significantly outperforms the other losses.

\subsubsection{ImageNet}

ImageNet is the gold-standard dataset for image classification tasks \citep{Russakovsky2015ImageNetLS}.
It has 1000 class labels and 1.2 million images.

We generate our distance metric over the image labels using a pretrained fastText model \citep{bojanowski2017enriching}.
We use fastText rather than word2vec because many of the class labels contain words not in the word2vec models' vocabulary;
since fastText is naturally able to handle out-of-vocabulary words,
there is not need to discard class labels like we did for CIFAR100.
Many ImageNet class label names contain multiple words,
and for these classes we generate the embedding with a simple average of the fastText embedding over all words.

We again train a ResNet50 model \citep{He2016DeepRL} using standard hyperparameters, replacing only the last layer of the model.
As with CIFAR100, Table \ref{table:results} shows that the tree loss significantly outperforms the other losses.
Since the publication of ResNet50 in 2016,
there have been many newer network architectures with better performance on ImageNet \citep[e.g.][]{howard2017mobilenets,huang2017densely,pmlr-v97-tan19a}.
We unfortunately did not have sufficient computational resources to train these more modern models with the tree loss,
and we emphasize that we are not trying to compete directly with these network architectures to achieve state-of-the-art performance.
Instead, our goal is to show that for a given network architecture,
replacing the cross entropy loss with the tree loss results in improved performance.

\subsubsection{Twitter Data}

We now consider the problem of predicting the emotions of twitter text.
This is a very different domain than the image problems considered above,
and demonstrates that the tree loss works across many domains.
We use the dataset collected by \citep{stoikos2020multilingual} for analyzing emotional response to Covid19 on Twitter.
This dataset contains all geolocated tweets written in any language sent over the 6 month period between 1 January 2020 and 1 July 2020.
Tweets in the dataset are preprocessed to have all emojis, usernames, and urls removed from the text,
and then the goal is to predict the emoji given only the remaining text.
Since most emojis represent emotions,
the task of emoji prediction serves as a proxy for emotion prediction.

We generate our distance metric over the class labels using the pretrained emoji2vec model \citep{Eisner2016emoji2vecLE},
which associates each emoji with a vector embedding.
We then follow the procedure in \citet{stoikos2020multilingual} to train multilingual BERT models \citep{Feng2020LanguageagnosticBS} with the last layers replaced by the three different loss functions.
Table \ref{table:results} shows the tree loss significantly outperforming the baseline models.
%Furthermore, the original BERTmoticon model proposed in \citet{stoikos2020multilingual} is only able to predict 80 emoji categories and ignores all other emoji,
%a restriction they found necessary due to the sample complexity of including more emoji in the classsification problem.
%But our model using the tree loss is able to predict over 1600 different emoji and still achieves better accuracy than the original BERTmoticon.

\section{Conclusion}
The tree loss is a drop-in replacement for the cross entropy loss for multiclass classification problems.
It can take advantage of background knowledge about the underlying class structure in order to improve the convergence rate of SGD.

Both our theoretical and empirical results show that the tree loss improves performance over the cross entropy loss in a wide range of circumstances.
Remarkably, we observe that the tree loss improves performance even when our assumptions about the underlying class structure are perfectly wrong (see Section \ref{sec:synth:eps}).
We therefore suggest that you replace the cross entropy loss with the tree loss on all of your multiclass problems.
%there are no downsides there is essentially no downside to using the tree loss as a drop-in replacement for the cross entropy loss.
%Even if you have no a-priori knowledge about the class structure,
%and are forced to use a random tree,
%the tree loss can still improve classification accuracy.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\bibliographystyle{plainnat}
\bibliography{paper}
% \begin{abstract}
%   The Abstract paragraph should be indented 0.25 inch (1.5 picas) on
%   both left and right-hand margins. Use 10~point type, with a vertical
%   spacing of 11~points. The \textbf{Abstract} heading must be centered,
%   bold, and in point size 12. Two line spaces precede the
%   Abstract. The Abstract must be limited to one paragraph.
% \end{abstract}

% \section{GENERAL FORMATTING INSTRUCTIONS}

% The camera-ready versions of the accepted papers are 8 pages,
% plus any additional pages needed for references.

% Papers are in 2 columns with the overall line width of 6.75~inches (41~picas).
% Each column is 3.25~inches wide (19.5~picas).  The space
% between the columns is .25~inches wide (1.5~picas).  The left margin is 0.88~inches (5.28~picas).
% Use 10~point type with a vertical spacing of
% 11~points. Please use US Letter size paper instead of A4.

% Paper title is 16~point, caps/lc, bold, centered between 2~horizontal rules.
% Top rule is 4~points thick and bottom rule is 1~point thick.
% Allow 1/4~inch space above and below title to rules.

% Author descriptions are center-justified, initial caps.  The lead
% author is to be listed first (left-most), and the Co-authors are set
% to follow.  If up to three authors, use a single row of author
% descriptions, each one center-justified, and all set side by side;
% with more authors or unusually long names or institutions, use more
% rows.

% Use one-half line space between paragraphs, with no indent.

% \section{FIRST LEVEL HEADINGS}

% First level headings are all caps, flush left, bold, and in point size
% 12. Use one line space before the first level heading and one-half line space
% after the first level heading.

% \subsection{Second Level Heading}

% Second level headings are initial caps, flush left, bold, and in point
% size 10. Use one line space before the second level heading and one-half line
% space after the second level heading.

% \subsubsection{Third Level Heading}

% Third level headings are flush left, initial caps, bold, and in point
% size 10. Use one line space before the third level heading and one-half line
% space after the third level heading.

% \paragraph{Fourth Level Heading}

% Fourth level headings must be flush left, initial caps, bold, and
% Roman type.  Use one line space before the fourth level heading, and
% place the section text immediately after the heading with no line
% break, but an 11 point horizontal space.

% %%%
% \subsection{Citations, Figure, References}


% \subsubsection{Citations in Text}

% Citations within the text should include the author's last name and
% year, e.g., (Cheesman, 1985). 
% %Apart from including the author's last name and year, citations can follow any style, as long as the style is consistent throughout the paper.  
% Be sure that the sentence reads
% correctly if the citation is deleted: e.g., instead of ``As described
% by (Cheesman, 1985), we first frobulate the widgets,'' write ``As
% described by Cheesman (1985), we first frobulate the widgets.''


% The references listed at the end of the paper can follow any style as long as it is used consistently.

% %Be sure to avoid
% %accidentally disclosing author identities through citations.

% \subsubsection{Footnotes}

% Indicate footnotes with a number\footnote{Sample of the first
%   footnote.} in the text. Use 8 point type for footnotes. Place the
% footnotes at the bottom of the column in which their markers appear,
% continuing to the next column if required. Precede the footnote
% section of a column with a 0.5 point horizontal rule 1~inch (6~picas)
% long.\footnote{Sample of the second footnote.}

% \subsubsection{Figures}

% All artwork must be centered, neat, clean, and legible.  All lines
% should be very dark for purposes of reproduction, and art work should
% not be hand-drawn.  Figures may appear at the top of a column, at the
% top of a page spanning multiple columns, inline within a column, or
% with text wrapped around them, but the figure number and caption
% always appear immediately below the figure.  Leave 2 line spaces
% between the figure and the caption. The figure caption is initial caps
% and each figure should be numbered consecutively.

% Make sure that the figure caption does not get separated from the
% figure. Leave extra white space at the bottom of the page rather than
% splitting the figure and figure caption.
% \begin{figure}[h]
% \vspace{.3in}
% \centerline{\fbox{This figure intentionally left non-blank}}
% \vspace{.3in}
% \caption{Sample Figure Caption}
% \end{figure}

% \subsubsection{Tables}

% All tables must be centered, neat, clean, and legible. Do not use hand-drawn tables.
% Table number and title always appear above the table.
% See Table~\ref{sample-table}.

% Use one line space before the table title, one line space after the table title,
% and one line space after the table. The table title must be
% initial caps and each table numbered consecutively.

% \begin{table}[h]
% \caption{Sample Table Title} \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \textbf{PART}  &\textbf{DESCRIPTION} \\
% \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}

% \section{SUPPLEMENTARY MATERIAL}

% If you need to include additional appendices during submission, you can include them in the supplementary material file.
% You can submit a single file of additional supplementary material which may be either a pdf file (such as proof details) or a zip file for other formats/more files (such as code or videos). 
% Note that reviewers are under no obligation to examine your supplementary material. 
% If you have only one supplementary pdf file, please upload it as is; otherwise gather everything to the single zip file.

% You must use \texttt{aistats2022.sty} as a style file for your supplementary pdf file and follow the same formatting instructions as in the main paper. 
% The only difference is that it must be in a \emph{single-column} format.
% You can use \texttt{supplement.tex} in our starter pack as a starting point.
% Alternatively, you may append the supplementary content to the main paper and split the final PDF into two separate files.

% \section{SUBMISSION INSTRUCTIONS}

% To submit your paper to AISTATS 2022, please follow these instructions.

% \begin{enumerate}
%     \item Download \texttt{aistats2022.sty}, \texttt{fancyhdr.sty}, and \texttt{sample\_paper.tex} provided in our starter pack. 
%     Please, do not modify the style files as this might result in a formatting violation.
    
%     \item Use \texttt{sample\_paper.tex} as a starting point.
%     \item Begin your document with
%     \begin{flushleft}
%     \texttt{\textbackslash documentclass[twoside]\{article\}}\\
%     \texttt{\textbackslash usepackage\{aistats2022\}}
%     \end{flushleft}
%     The \texttt{twoside} option for the class article allows the
%     package \texttt{fancyhdr.sty} to include headings for even and odd
%     numbered pages.
%     \item When you are ready to submit the manuscript, compile the latex file to obtain the pdf file.
%     \item Check that the content of your submission, \emph{excluding} references, is limited to \textbf{8 pages}. The number of pages containing references alone is not limited.
%     \item Upload the PDF file along with other supplementary material files to the CMT website.
% \end{enumerate}

% \subsection{Camera-ready Papers}

% %For the camera-ready paper, if you are using \LaTeX, please make sure
% %that you follow these instructions.  
% % (If you are not using \LaTeX,
% %please make sure to achieve the same effect using your chosen
% %typesetting package.)

% If your papers are accepted, you will need to submit the camera-ready version. Please make sure that you follow these instructions:
% \begin{enumerate}
%     %\item Download \texttt{fancyhdr.sty} -- the
%     %\texttt{aistats2022.sty} file will make use of it.
%     \item Change the beginning of your document to
%     \begin{flushleft}
%     \texttt{\textbackslash documentclass[twoside]\{article\}}\\
%     \texttt{\textbackslash usepackage[accepted]\{aistats2022\}}
%     \end{flushleft}
%     The option \texttt{accepted} for the package
%     \texttt{aistats2022.sty} will write a copyright notice at the end of
%     the first column of the first page. This option will also print
%     headings for the paper.  For the \emph{even} pages, the title of
%     the paper will be used as heading and for \emph{odd} pages the
%     author names will be used as heading.  If the title of the paper
%     is too long or the number of authors is too large, the style will
%     print a warning message as heading. If this happens additional
%     commands can be used to place as headings shorter versions of the
%     title and the author names. This is explained in the next point.
%     \item  If you get warning messages as described above, then
%     immediately after $\texttt{\textbackslash
%     begin\{document\}}$, write
%     \begin{flushleft}
%     \texttt{\textbackslash runningtitle\{Provide here an alternative
%     shorter version of the title of your paper\}}\\
%     \texttt{\textbackslash runningauthor\{Provide here the surnames of
%     the authors of your paper, all separated by commas\}}
%     \end{flushleft}
%     Note that the text that appears as argument in \texttt{\textbackslash
%       runningtitle} will be printed as a heading in the \emph{even}
%     pages. The text that appears as argument in \texttt{\textbackslash
%       runningauthor} will be printed as a heading in the \emph{odd}
%     pages.  If even the author surnames do not fit, it is acceptable
%     to give a subset of author names followed by ``et al.''

%     %\item Use the file sample\_paper.tex as an example.

%     \item The camera-ready versions of the accepted papers are 8
%       pages, plus any additional pages needed for references.

%     \item If you need to include additional appendices,
%       you can include them in the supplementary
%       material file.

%     \item Please, do not change the layout given by the above
%       instructions and by the style file.

% \end{enumerate}

% \subsubsection*{Acknowledgements}
% All acknowledgments go at the end of the paper, including thanks to reviewers who gave useful comments, to colleagues who contributed to the ideas, and to funding agencies and corporate sponsors that provided financial support. 
% To preserve the anonymity, please include acknowledgments \emph{only} in the camera-ready papers.


% \subsubsection*{References}

% References follow the acknowledgements.  Use an unnumbered third level
% heading for the references section.  Please use the same font
% size for references as for the body of the paper---remember that
% references do not count against your page length total.

% \begin{thebibliography}{}
% \setlength{\itemindent}{-\leftmargin}
% \makeatletter\renewcommand{\@biblabel}[1]{}\makeatother
% \bibitem{} J.~Alspector, B.~Gupta, and R.~B.~Allen (1989).
%     \newblock Performance of a stochastic learning microchip.
%     \newblock In D. S. Touretzky (ed.),
%     \textit{Advances in Neural Information Processing Systems 1}, 748--760.
%     San Mateo, Calif.: Morgan Kaufmann.

% \bibitem{} F.~Rosenblatt (1962).
%     \newblock \textit{Principles of Neurodynamics.}
%     \newblock Washington, D.C.: Spartan Books.

% \bibitem{} G.~Tesauro (1989).
%     \newblock Neurogammon wins computer Olympiad.
%     \newblock \textit{Neural Computation} \textbf{1}(3):321--323.
% \end{thebibliography}

\ignore{
\appendix
\section{Related loss functions}

Many alternatives to the cross entropy loss have been proposed.

The tree loss is superficially related to the hierarchical softmax (HSM) because both losses are defined based on tree structures.
\cite{morin2005hierarchical} introduced the HSM to speed up the computational time of neural network training,
but the tree loss is used to improve sample efficiency rather than computational efficiency.

\cite{mikolov2013distributed} introduce the word2vec model and experiment with using both the hierarchical softmax and cross entropy with negative sampling.
They conclude that negative sampling has the best performance both computationally and statistically.
Since the tree loss is a simple extension of the cross entropy loss,
negative sampling can be used to speed up the cross entropy loss as well.

\cite{bojanowski2017enriching} introduce the fastText extension of the word2vec model, which uses sub-word information to improve sample efficiency.
They do not provide any theoretical justification for why their technique should improve sample efficiency,
but their technique is computationally very similar to the tree loss,
and so our technique may be able to provide the first theoretical justification for their work.

HSM:
\begin{equation}
    \label{eq:xentropy}
    \ell(W;(\x,y)) = - \log \prod_{k\in P_y}\frac {\exp(-\trans\w_k \x)}{j\in L_k \exp(-\trans \w_j \x)}
\end{equation}
where $P_y$ is path from root to node of Huffman Tree,
$L_k$ are nodes on the level where $k$ are


The hierarchical softmax loss aims to improve computation efficiency. 
\cite{Peng2017IncrementallyLT} introduce a training method incrementally train the hierarchical softmax function for neural network language models.
They also provide a new stochastic gradient based method to update all the word vectors and their experimental results show the incremental training can save a lot of time.
\cite{Jiang2017ExplorationOT} explore the tree-based hierarchical softmax method and reform its architecture, making it compatible with modern GPUs.
\cite{Yang2017OptimizeHS} gives a theoretic analysis on how should organize the hierarchical tree by treating the tree structure as a parameter of the training objective function.
They propose SemHuff, a new tree constructing scheme based on adjusting the Huffman tree with word similarity knowledge.
Their experiments results also show that word embeddings trained with optimized hierarchical tree can give better results in various tasks.

\cite{Mohammed2018EffectivenessOH} compare the performance of normal softmax and hierarchical softmax in large scale classification tasks. 
Normal softmax is computationally expensive for large scale datasets with large number of possible outputs.
Their experimental results show that normal softmax and hierarchical softmax underperforms on datasets with large number of labels.
That is also one of our motivations of the tree loss function. 
Compared to classic cross entropy loss function, the tree loss function performs better on datasets with large number of labels.
Extreme multi-label classification is difficult, but organizing labels as a tree can handle this problem. 

% \fixme{Write a detailed description of the sim loss}

SimLoss \cite{Kobs2020SimLossCS} is similarity based loss function which incorporates class similarities to support the training of neural network classifiers.
Cross entropy loss assumes only one class is correct and punishes every misclassification, while Simloss indicates classes have similarities and classifying a target as a similar class is better.
When calculating the loss function, instead of only taking the probability vector at the target index Simloss also adds the product of similarity between target and other classes and probability vector at the corresponding class. 
Non zero values of similarity matrix lead to smaller losses when the network gives a high score to classes similar to the correct one. 
It would allow the classifier to make less severe mistakes as it learns to predict similar classes.
However, SimLoss does not have any theoretical justification to prove why it can outperform cross entropy.
SimLoss has a hyperparameter lower bound that controls the minimal class similarity that should have an impact on the network punishment.
Any similarities below lower bound will be all zeros.
It implies SimLoss would cost more time to fine tune.
Our tree loss define a tree structure which can be obtained automatically and it is time-saving.
\begin{align}
    L_{Simloss}&=-\frac{1}{N}\log(\sum_{k=1}^{C}S_{y_i,k}\cdot p_i[k])\\
    % L_{Simloss}=-\frac{1}{N}\log(\sum_{k=1}^{C}S_{y_i,k}\cdot\frac {\exp(-\trans\w_{y_i} \x)}{\sum_{j=1}^C \exp(-\trans \w_j \x)})
    p_i&=\frac {\exp(-\trans\w_{y_i} \x)}{\sum_{j=1}^C \exp(-\trans \w_j \x)}
\end{align}

}
\end{document}
