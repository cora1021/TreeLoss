
@inproceedings{morin2005hierarchical,
  title={Hierarchical probabilistic neural network language model.},
  author={Morin, Frederic and Bengio, Yoshua},
  booktitle={Aistats},
  volume={5},
  pages={246--252},
  year={2005},
  organization={Citeseer}
}

@article{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1310.4546},
  year={2013}
}

@article{bojanowski2017enriching,
  title={Enriching word vectors with subword information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={5},
  pages={135--146},
  year={2017},
  publisher={MIT Press}
}

% HSM
@inproceedings{Peng2017IncrementallyLT,
  title={Incrementally Learning the Hierarchical Softmax Function for Neural Language Models},
  author={Hao Peng and Jianxin Li and Y. Song and Yaopeng Liu},
  booktitle={AAAI},
  year={2017}
}

@article{Mohammed2018EffectivenessOH,
  title={Effectiveness of Hierarchical Softmax in Large Scale Classification Tasks},
  author={Abdul Arfat Mohammed and Venkatesh Umaashankar},
  journal={2018 International Conference on Advances in Computing, Communications and Informatics (ICACCI)},
  year={2018},
  pages={1090-1094}
}

@inproceedings{Wydmuch2018ANG,
  title={A no-regret generalization of hierarchical softmax to extreme multi-label classification},
  author={Marek Wydmuch and Kalina Jasinska and Mikhail Kuznetsov and R. Busa-Fekete and K. Dembczynski},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{Jiang2017ExplorationOT,
  title={Exploration of Tree-based Hierarchical Softmax for Recurrent Language Models},
  author={Nan Jiang and Wenge Rong and M. Gao and Yikang Shen and Z. Xiong},
  booktitle={IJCAI},
  year={2017}
}

@article{Yang2017OptimizeHS,
  title={Optimize Hierarchical Softmax with Word Similarity Knowledge},
  author={Zhixuan Yang and Chong Ruan and Caihua Li and J. Hu},
  journal={Polytech. Open Libr. Int. Bull. Inf. Technol. Sci.},
  year={2017},
  volume={55},
  pages={11-16}
}

@article{Grave2017EfficientSA,
  title={Efficient softmax approximation for GPUs},
  author={Edouard Grave and Armand Joulin and Moustapha Ciss{\'e} and David Grangier and H. J{\'e}gou},
  journal={ArXiv},
  year={2017},
  volume={abs/1609.04309}
}

@article{Kobs2020SimLossCS,
  title={SimLoss: Class Similarities in Cross Entropy},
  author={Konstantin Kobs and M. Steininger and Albin Zehe and Florian Lautenschlager and A. Hotho},
  journal={ArXiv},
  year={2020},
  volume={abs/2003.03182}
}

@article{Feng2020LanguageagnosticBS,
  title={Language-agnostic BERT Sentence Embedding},
  author={Fangxiaoyu Feng and Yin-Fei Yang and Daniel Matthew Cer and N. Arivazhagan and Wei Wang},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.01852}
}

@article{Izbicki2019GeolocatingTI,
  title={Geolocating Tweets in any Language at any Location},
  author={Michael Izbicki and Evangelos E. Papalexakis and V. Tsotras},
  journal={Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
  year={2019}
}

@article{Eisner2016emoji2vecLE,
  title={emoji2vec: Learning Emoji Representations from their Description},
  author={Ben Eisner and Tim Rockt{\"a}schel and Isabelle Augenstein and Matko Bosnjak and S. Riedel},
  journal={ArXiv},
  year={2016},
  volume={abs/1609.08359}
}

@inproceedings{Mikolov2013EfficientEO,
  title={Efficient Estimation of Word Representations in Vector Space},
  author={Tomas Mikolov and Kai Chen and G. Corrado and J. Dean},
  booktitle={ICLR},
  year={2013}
}

@article{He2016DeepRL,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={770-778}
}

@inproceedings{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={J. Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={NAACL},
  year={2019}
}

@article{Russakovsky2015ImageNetLS,
  title={ImageNet Large Scale Visual Recognition Challenge},
  author={Olga Russakovsky and J. Deng and Hao Su and J. Krause and S. Satheesh and S. Ma and Zhiheng Huang and A. Karpathy and A. Khosla and Michael S. Bernstein and A. Berg and Li Fei-Fei},
  journal={International Journal of Computer Vision},
  year={2015},
  volume={115},
  pages={211-252}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

<<<<<<< HEAD
@inproceedings{Krizhevsky2009LearningML,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Alex Krizhevsky},
  year={2009}
}
=======
@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@article{jentzen2020lower,
  title={Lower error bounds for the stochastic gradient descent optimization algorithm: Sharp convergence rates for slowly and fast decaying learning rates},
  author={Jentzen, Arnulf and Von Wurstemberger, Philippe},
  journal={Journal of Complexity},
  volume={57},
  pages={101438},
  year={2020},
  publisher={Elsevier}
}

@article{nguyen2018tight,
  title={Tight dimension independent lower bound on the expected convergence rate for diminishing step sizes in SGD},
  author={Nguyen, Phuong Ha and Nguyen, Lam M and van Dijk, Marten},
  journal={NeurIPS},
  year={2019}
}

@inproceedings{izbicki2015faster,
  title={Faster cover trees},
  author={Izbicki, Mike and Shelton, Christian},
  booktitle={International Conference on Machine Learning},
  pages={1162--1170},
  year={2015},
  organization={PMLR}
}
@phdthesis{izbickithesis,
    title    = {Divide and Conquer Algorithms for Machine Learning},
    school   = {University of California, Riverside},
    author   = {Izbicki, Mike},
    year     = {2017}
}

@inproceedings{beygelzimer2006cover,
  title={Cover trees for nearest neighbor},
  author={Beygelzimer, Alina and Kakade, Sham and Langford, John},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={97--104},
  year={2006}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@InProceedings{pmlr-v97-tan19a,
  title = 	 {{E}fficient{N}et: Rethinking Model Scaling for Convolutional Neural Networks},
  author =       {Tan, Mingxing and Le, Quoc},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6105--6114},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/tan19a/tan19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/tan19a.html},
  abstract = 	 {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flower (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.}
}

@article{howard2017mobilenets,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

@inproceedings{stoikos2020multilingual,
  title={Multilingual Emoticon Prediction of Tweets about COVID-19},
  author={Stoikos, Stefanos and Izbicki, Mike},
  booktitle={Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media},
  pages={109--118},
  year={2020}
}

@article{natarajan1989learning,
  title={On learning sets and functions},
  author={Natarajan, Balas K},
  journal={Machine Learning},
  volume={4},
  number={1},
  pages={67--97},
  year={1989},
  publisher={Springer}
}

@incollection{vapnik2015uniform,
  title={On the uniform convergence of relative frequencies of events to their probabilities},
  author={Vapnik, Vladimir N and Chervonenkis, A Ya},
  booktitle={Theory of Probability and Its Applications},
  year={1971}
}

@inproceedings{zhang2018generalized,
  title={Generalized cross entropy loss for training deep neural networks with noisy labels},
  author={Zhang, Zhilu and Sabuncu, Mert R},
  booktitle={32nd Conference on Neural Information Processing Systems (NeurIPS)},
  year={2018}
}

@article{sukhbaatar2014training,
  title={Training convolutional networks with noisy labels},
  author={Sukhbaatar, Sainbayar and Bruna, Joan and Paluri, Manohar and Bourdev, Lubomir and Fergus, Rob},
  journal={ICLR},
  year={2015}
}

@article{cesa2006incremental,
  title={Incremental algorithms for hierarchical classification},
  author={Cesa-Bianchi, Nicolo and Gentile, Claudio and Zaniboni, Luca},
  journal={The Journal of Machine Learning Research},
  volume={7},
  pages={31--54},
  year={2006},
  publisher={JMLR. org}
}

@article{wu2017hierarchical,
  title={A hierarchical loss and its problems when classifying non-hierarchically},
  author={Wu, Cinna and Tygert, Mark and LeCun, Yann},
  journal={PLoS ONE},
  year={2019}
}

@inproceedings{lapin2016loss,
  title={Loss functions for top-k error: Analysis and insights},
  author={Lapin, Maksim and Hein, Matthias and Schiele, Bernt},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1468--1477},
  year={2016}
}

@article{liu2017easy,
  title={An easy-to-hard learning paradigm for multiple classes and multiple labels},
  author={Liu, Weiwei and Tsang, Ivor W and M{\"u}ller, Klaus-Robert},
  journal={Journal of Machine Learning Research},
  year={2017}
}

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}
<<<<<<< HEAD

@article{rakhlin2011making,
  title={Making gradient descent optimal for strongly convex stochastic optimization},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  journal={ICML},
  year={2012}
}

@article{zou2021benign,
  title={Benign overfitting of constant-stepsize sgd for linear regression},
  author={Zou, Difan and Wu, Jingfeng and Braverman, Vladimir and Gu, Quanquan and Kakade, Sham M},
  journal={arXiv preprint arXiv:2103.12692},
  year={2021}
}

@article{dasgupta2003elementary,
  title={An elementary proof of a theorem of Johnson and Lindenstrauss},
  author={Dasgupta, Sanjoy and Gupta, Anupam},
  journal={Random Structures \& Algorithms},
  volume={22},
  number={1},
  pages={60--65},
  year={2003},
  publisher={Wiley Online Library}
}

@article{bilal2018,
   title={Do Convolutional Neural Networks Learn Class Hierarchy?},
   volume={24},
   ISSN={1077-2626},
   url={http://dx.doi.org/10.1109/TVCG.2017.2744683},
   DOI={10.1109/tvcg.2017.2744683},
   number={1},
   journal={IEEE Transactions on Visualization and Computer Graphics},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Bilal, Alsallakh and Jourabloo, Amin and Ye, Mao and Liu, Xiaoming and Ren, Liu},
   year={2018},
   month={Jan},
   pages={152â€“162}
}

@phdthesis{akata2014contributions,
  title={Contributions to large-scale learning for image classification},
  author={Akata, Zeynep},
  year={2014},
  school={Universit{\'e} de Grenoble}
}

@phdthesis{deng2012large,
  title={Large scale visual recognition},
  author={Deng, Jia},
  year={2012},
  school={Princeton University}
}

@article{reddi2019convergence,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={ICLR},
  year={2018}
}
